---
title: "Intro to Text Mining"
author: "Pantea Ferdosian, Kevin Hoffman, Luke Moles, Marissa Shand"
output:
 html_document:
   toc: TRUE
   theme: united
   toc_depth: 3
   number_sections: TRUE
   df_print: paged
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Text Mining Motivation

## **Need for Text Mining**:

The amount of data that we produce every day is truly astonishing. With the evolution of communication through social media, we generate tons and tons of data. Out of all of this generated data, only around $20\%$ are Structured and well formatted. The remaining of the data is Unstructured Data For instance, the bulk of emails we send, the text messages and the comments in social media are examples of Unstructured Data.
These Unstructured data may be analyzed and mined in order to extract useful information. 


## What is "Text Mining" and where is it used? 

Text Mining is basically the process of extracting meaningful information from natural language text. 

A few examples of how we use text mining every day:

* Auto-complete feature
* Spam Detection
* Predictive Typing
* Spell Checker
    
##  Terminologies In Text Mining:

**Tokenization: ** The process of splitting the whole data (corpus) into smaller chunks is known as tokenization. (i.e. sentences into words)

**Remove Stop Words:** Stop words that are words that are very commonly used such as "although", "really", "from", etc. By removing such words in a language, we may focus on the important words instead.

**Document Term Matrix or DTM:** A matrix containing the terms that appear in a collection of documents and the frequency of the terms, where rows represent the documents and columns represent the words (terms). 


# Working with text data

"A **token** is a meaningful unit of text, such as wa word, that we are interested in using for analysis, and tokenization is the process of splitting test into tokens" [1]. Can be a word, n-gram, sentence or paragraph.

## Packages we will be using 

```{r, warning = FALSE, message = FALSE}
library(tidyverse)
library(tidytext)
library(gutenbergr)
library(dplyr)
library(igraph)
library(ggraph)
#install.packages('textdata')
```

## Exploring Project Gutenberg

"Project Gutenberg is an online library of free eBooks. Project Gutenberg was the first provider of free electronic books, or eBooks." [2]

```{r}
## Find books by Charles Dickens
dickens <- gutenberg_works(author == 'Dickens, Charles')
dim(dickens)
head(dickens)

## We will be working with A Tale of Two Cities, which has id 98
two_cities <- gutenberg_download(98)
head(two_cities)

## There are three books that make up this book
## Get book number
two_cities <- two_cities %>% mutate(book = cumsum(str_detect(text, regex("^Book the"))),
                                    linenumber = row_number())

## For each book get the linenumber in the book, and the chapter
## Roman numerals: https://www.oreilly.com/library/view/regular-expressions-cookbook/9780596802837/ch06s09.html
two_cities <- two_cities %>% group_by(book) %>% 
   mutate(book_linenumber = row_number(),
          chapter = cumsum(str_detect(text, regex("^(?=[MDCLXVI])M*(C[MD]|D?C{0,3})(X[CL]|L?X{0,3})(I[XV]|V?I{0,3})[.]")))) %>%
   ungroup()

## Convert to tidy text
tidy_two_cities <- two_cities %>% unnest_tokens(word, text)
head(tidy_two_cities)
```
## Stop words

```{r}
## Error arose: https://stackoverflow.com/questions/9221310/r-debugging-only-0s-may-be-mixed-with-negative-subscripts
data(stop_words)

stop_two_cities <- tidy_two_cities %>% anti_join(stop_words, by = c("word" = "word"))

## How many words were removed?
dim(tidy_two_cities)[1] - dim(stop_two_cities)[1]
```


# Sentiment Analysis

Now that we have a book to work with, how do we determine the sentiment of the words in this book? 

## Sentiment Lexicons 

Within the tidytext library there are three lexicons for use:

1. AFINN - assigns each token a sentiment between -5 and 5
   * Gives a sense of degree of positivity or negativity 

2. Bing - groups words into a positive or negative category

3. Loughran - groups words into one of 6 different sentiment categories: {Constraining, Litigious, Negative, Positive, Superfluous, Uncertainty}
   * Primarily used for financial text

4. NRC - groups words into one of 10 different sentiment categories: {Anger, Anticipation, Disgust, Fear, Joy, Negative, Positive, Sadness, Surprise, Trust}

   
## Working with the sentiment lexicons

Because words can have different meanings depending on the context, words can fall into different sentiment categories so there are duplicates in the datasets.

```{r, message = FALSE}
## Sentiment lexicons
afinn <- get_sentiments('afinn')
afinn %>% 
  group_by(word) %>% 
  summarize(ndups = n_distinct(value)) %>% 
  filter(ndups>1)

bing <- get_sentiments('bing')
bing %>% 
  group_by(word) %>% 
  summarize(ndups = n_distinct(sentiment)) %>% 
  filter(ndups>1)

loughran <- get_sentiments('loughran')
loughran %>% 
  group_by(word) %>% 
  summarize(ndups = n_distinct(sentiment)) %>% 
  filter(ndups>1)

nrc <- get_sentiments('nrc')
nrc %>% 
  group_by(word) %>% 
  summarize(ndups = n_distinct(sentiment)) %>% 
  filter(ndups>1)
```

## Comparing AFINN and Bing

Let's compare AFINN and Bing. If AFINN value is greater than 0, assign it a positive value else assign negative value. They disagree on 17 words which makes up 0.01% of the data. Therefore AFINN and Bing generally agree with each other

```{r}
## Join afinn and bing
afinn_bing <- afinn %>% inner_join(bing, by = c('word' = 'word')) %>% 
   rename('bing_sentiment' = 'sentiment') %>%
   mutate(afinn_sentiment = ifelse(value >= 0, 'positive', 'negative'),
          same = afinn_sentiment == bing_sentiment)

## How many words do they disagree on?
dim(afinn_bing %>% filter(same == FALSE))[1]
dim(afinn_bing %>% filter(same == FALSE))[1]/dim(afinn_bing)[1]
```


## Overall sentiment of A Tale of Two Cities

If we just look at positive or negative categories to determine the overall sentiment of the book, we see that the overall sentiment is 0.366 which 

Can calculate using 

Overall sentiment = \frac{positive words}{total words}

```{r}
## Join sentiments with Tale of Two Cities
bing_two_cities <- stop_two_cities %>% left_join(bing, by = c('word' = 'word')) %>% rename('bing' = 'sentiment')

## How many words have an associated bing sentiment?
dim(bing_two_cities %>% filter(!is.na(bing)))[1]
dim(bing_two_cities %>% filter(!is.na(bing)))[1]/dim(bing_two_cities)[1]

## Filter for words that have a sentiment value
bing_two_cities <- bing_two_cities %>% filter(!is.na(bing))
```

```{r}
dim(bing_two_cities %>% filter(bing == "positive"))[1]/dim(bing_two_cities)[1]
```

```{r}
## Join sentiments with Tale of Two Cities
afinn_two_cities <- stop_two_cities %>% left_join(afinn, by = c('word' = 'word')) %>% rename('afinn' = 'value')

## How many words have an associated afinn sentiment?
dim(afinn_two_cities %>% filter(!is.na(afinn)))[1]
dim(afinn_two_cities %>% filter(!is.na(afinn)))[1]/dim(afinn_two_cities)[1]

## Filter for words that have a sentiment value
afinn_two_cities <- afinn_two_cities %>% filter(!is.na(afinn))
```

```{r}
mean(afinn_two_cities$afinn)
```



## Sentiment broken down by book and chapter: analyzing sentiment change over time

```{r}
## Sentiment of each book
afinn_two_cities %>% 
   group_by(book) %>% 
   summarize(average_sentiment = mean(afinn)) %>% 
   filter(book > 0)

## Sentiment of each chapter of each book
afinn_two_cities %>% 
   group_by(book, chapter) %>% 
   summarize(average_sentiment = mean(afinn)) %>% 
   filter(book > 0)

## Look at graphical representation of sentiment over time
afinn_two_cities %>% group_by(book, book_linenumber) %>% 
   summarize(average_sentiment = mean(afinn)) %>% 
   filter(book > 0) %>% 
   ggplot() + 
   geom_col(aes(x = book_linenumber, y = average_sentiment, fill = book), show.legend = FALSE) + 
   facet_wrap(~book, ncol = 1, scales = 'free_x')
```

```{r}
## Sentiment of each book
bing_two_cities %>% 
   group_by(book) %>% 
   summarize(num_pos = sum(bing == "positive"),
             num_neg = sum(bing == "ngeative"),
             sentiment = num_pos - num_neg, 
             average_sentiment = sum(bing == "positive")/n()) %>% 
   filter(book > 0)

## Sentiment of each chapter of each book
bing_two_cities %>% 
   group_by(book, chapter) %>% 
   summarize(num_pos = sum(bing == "positive"),
             num_neg = sum(bing == "negative"),
             sentiment = num_pos - num_neg,
             average_sentiment = sum(bing == "positive")/n()) %>% 
   filter(book > 0)

## Look at graphical representation of sentiment over time
bing_two_cities %>% group_by(book, book_linenumber) %>% 
   summarize(num_pos = sum(bing == "positve"),
             num_neg = sum(bing == "negative"),
             sentiment = num_pos - num_neg,
             average_sentiment = sum(bing == "positive")/n()) %>% 
   filter(book > 0) %>% 
   ggplot() + 
   geom_col(aes(x = book_linenumber, y = sentiment, fill = book), show.legend = FALSE) + 
   facet_wrap(~book, ncol = 1, scales = 'free_x')
```


# TF-IDF

**How should quantify what a document is about?**


* TF term is short for *term frequency* which indicates how frequently a word appears in a document. The document would most likely include unimportant words which we classified earlier as *stop words*, but removing the stop words would not be an efficient approach in this case because some of these words might be more important in some documents than others. 

* IDF term or *inverse document frequency* defined as :$idf(term) = ln(\frac{n_{documenta}}{n_{documents\ containing\ term}})$. This method adjust the weights and balances the usage of words in documents. This implies that it decreases the weight for widely used words and increases the weight for words that did not appear a lot.

* TF-IDF is short for *term frequencyâ€“inverse document frequency*, which is a statistical measure that indicates how important a word is to a document in a collection. This is obtained by multiplying the 2 mentioned quantities, which would indicate "the frequency of a term adjusted for how rarely it is used".

## Top 10 words with highest tf-idf values in Charles Dicken's A Tale of Two Cities (By chapters)

```{r}
tidy_two_cities %>%
    count(chapter, word, sort = TRUE) %>%
    bind_tf_idf(word, chapter, n) %>%
    arrange(-tf_idf) %>%
    group_by(chapter) %>%
    top_n(10) %>%
    ungroup %>%
    mutate(word = reorder(word, tf_idf)) %>%
    ggplot(aes(word, tf_idf, fill = chapter)) +
    geom_col(show.legend = FALSE) +
    facet_wrap(~ chapter, scales = "free") +
    coord_flip()
```

As shown below, the $tf$-$idf$ values are not different after we remove the stop words because the $idf$ values of our stop words are very small, as they appear in all chapters.


## After removing Stop words

```{r}
stop_two_cities %>%
    count(chapter, word, sort = TRUE) %>%
    bind_tf_idf(word, chapter, n) %>%
    arrange(-tf_idf) %>%
    group_by(chapter) %>%
    top_n(10) %>%
    ungroup %>%
    mutate(word = reorder(word, tf_idf)) %>%
    ggplot(aes(word, tf_idf, fill = chapter)) +
    geom_col(show.legend = FALSE) +
    facet_wrap(~ chapter, scales = "free") +
    coord_flip()
```

# Sequences of Words: n-grams

It is sometimes useful to look at groups of words in order to understand how they can relate and build off one another. The n-grams of a passage are all the length $n$ sequences of words. For example, the bi-grams of "The quick brown fox jumped" are:

* the quick
* quick brown
* brown fox
* fox jumped

and the tri-grams are:

* the quick brown
* quick brown fox
* brown fox jumped

Notice that these n-grams can contain stopwords, so it is often a good idea to filter out any results that include at least one. In practice, we can mine text for all n-grams with lengths between some specified min and max, but here we start by extracting the bigrams from *A Tale of Two Cities*.

```{r}
# download book text
ttc <- gutenberg_download(98)['text']
# remove blank spaces
ttc <- ttc[ttc$text!='',]
# remove header material
ttc <- ttc[53:nrow(ttc),]

# find all size 2 n-grams
# filter out pairs with stopwords
ngrams <- unnest_tokens(ttc, ngram, text, token='ngrams', n=2) %>%
  separate(col=ngram, into=c('first','second'), sep=' ') %>%
  filter(!(first %in% stop_words$word)) %>%
  filter(!(second %in% stop_words$word))

print(paste('There are', nrow(ngrams), 'bigrams without stopwords'))
```

Even after removing the n-grams that contain stopwords, there are still nearly 12,000 results. We can count the occurrences of each unique pair in order to reduce this number and get a better idea of what bigrams are important.

```{r}
# find the number of occurrences for each pair
# take the top 50
ngrams <- ngrams %>%
  group_by(first, second) %>%
  summarize(n=n()) %>%
  arrange(desc(n)) %>%
  head(50)

head(ngrams)
```
Since this text is from a novel, the most common pairs represent the names of characters who are frequently mentioned. Still, some other results may be interesting. We can visualize these by treating the bigrams as a directed graph.

```{r}
# make a directed graph of bigrams
g <- graph.data.frame(ngrams, directed=T)

# display graph
ggraph(g, layout='kk') +
  geom_edge_link(arrow=arrow(angle=20, type='closed', length=unit(0.1, 'inches')),
                 aes(color=n)) +
  geom_node_point() +
  geom_node_text(aes(label=name), size=2, vjust=1.5, hjust=1) +
  scale_edge_color_gradient(low='red', high='blue')
```

# Latent Direchlet Allocation

- Intro to Topic Modeling
   - similar to clustering numeric data
   - topics are clusters of words
   - goal is to discover latent patterns in documents
   - Examples
      - Apply it to a large batch of emails to understand what topics were discussed
      - Presidential speeches to identify themes
      - Collection of tweets from a group to identify what topics people tweet about
   - Latent Dirichlet allocation is an unsupervised method for finding topics in a collection of documents
      - probabilistic generative model
      - every document is a collection of topics
      - every topic is a collection of words
      - Documents are a mixture of topics ie document 1 is 60% of topic 1 and 40% of topic 2
      - Topics are a mixture of words ie 10% apples, 5% oranges, etc (maybe the topic is fruit)
   - Algorithm
      - From the paper:
         - Choose N from a poisson distribution
         - Choose $\Theta$ from a multinomial distribution
         - For each of the N words
            - Choose a topic $z_n$ from the multinomial($\theta$) distribution
            - Choose a word $w_n$ from $p(w_n|z_n,\beta)$, the probability of a word given the document
      - In easier terms maybe:
         - first, select k number of topics
         - randomly assign each word in each document in each topic
         - calculate proportion of words in document assigned to a topic
         - calculate proportion of words assigned to topic across all topics
         - Reassign word to new topic by using gibbes to sample the posterior
         - Repeat sampling for a number of draws
   - LDA Example: Randomly select 20 books from the top 10 authors gutenburg. How many authors are present?
      - authors are topics
      - books are documents
      - Tune k with https://cran.r-project.org/web/packages/ldatuning/vignettes/topics.html
         - Rather than fitting 10 models (k=1..10), use this package
         - Discuss hyperparemeter tuning
      - Identify and discuss one of the 4 criteria for evaluating each LDA model

# Set of popular authors
authors.popular <-
   c(
      "Dickens, Charles",
      "Austen, Jane",
      "Shelley, Mary Wollstonecraft",
      "Twain, Mark",
      "Doyle, Arthur Conan",
      "Wilde, Oscar",
      "Leech, John",
      "Hawthorne, Nathaniel",
      "Stevenson, Robert Louis",
      "Carroll, Lewis"
   )
```


Then we will get all books by these authors from the Gutenberg project. We will limit our selection to works written in English and ignore collection of works.

```{r}
books.authors.popular <- gutenberg_metadata %>%
   filter(
      author %in% authors.popular,
      language == "en", # Only English works
      !str_detect(title, "Works"),  # Ignore collections of works
      !str_detect(title, "Part"),
      !str_detect(title, "Volume"),
      !str_detect(title, "Chapters"),
      has_text,
      !str_detect(rights, "Copyright")
   ) %>%
   distinct(title, .keep_all = TRUE) %>% # Remove duplicate titles
   select(gutenberg_id, title, author)
```

We will select a random sample of 20 books and download them.

```{r}
set.seed(750)
books.selection <- books.authors.popular %>% sample_n(20)

# Download the 20 books
books.list <-
   books.selection$gutenberg_id %>% gutenberg_download(meta_fields = "title")
```

Now we have the text of all 20 books in a dataframe with the id and title. We will need to remove blank rows of the text and convert to tidytext.

```{r}
books.text <- books.list %>%
   filter(text != '') %>% # Remove blank lines
   select(-gutenberg_id) %>% # Drop the id
   group_by(title)  %>%
   unite(document, title)

words.by.book <- books.text %>%
   unnest_tokens(word, text)
```

Now we need to get the count of words so we can turn our data into a Document Term Matrix. We will also remove stop words. 

```{r}
word.counts <- words.by.book %>%
   anti_join(stop_words) %>%
   filter(
   !str_detect(word, "[0-9]"),
   !str_detect(word, "^_"),
   ) %>%
   count(document, word, sort = TRUE) %>%
   ungroup()

# Create a Document Term Matrix
books.dtm <- word.counts %>%
   cast_dtm(document, word, n)
```


We can run LDA with the LDA function from the topicmodels package. We will also have to select a k for the number of topics (authors). Let's try 5.

```{r}
library(topicmodels)
books.lda <- LDA(books.dtm, k = 5, control = list(seed = 555))
```

First, we can look at the probabilities of each word belonging to each topic. Convert the lda results to a tidy dataframe. This will have a row for each topic and term and its probability. Let's convert this into showing the 7 most likely words in each topic and graph it.
```{r}
books.topics <- tidy(books.lda, matrix = "beta")
words.top <- books.topics %>% 
   group_by(topic) %>%
  top_n(7, beta) %>%
  ungroup() %>%
   mutate(term = reorder_within(term, beta, topic))

ggplot(words.top, aes(beta, term, fill = factor(topic))) +
  geom_col(show.legend = FALSE) +
 facet_wrap(~ topic, scales = "free") +
 scale_y_reordered()

```

Let's look at some uncommon words or names in topic 1 such as "henry" and "stevenson"

```{r}
word.counts %>% filter(word == "henry")

word.counts %>% filter(word=="stevenson")
```
"henry" appears most often in "The Master of Ballantrae: A Winter's Tale" and "stevenson" appears most often in "Essays of Robert Louis Stevenson". We don't know this now, but both of these books are by Robert Louis Stevenson, a good sign. However we don't know how many authors we have so ideally we can run this LDA model for every possible number of topics (authors) 1 to 10 and have some evaluation metric to determine which is the best "fit". We can use the ldatuning package to do this for us.

```{r}
library(ldatuning)
topics.results <- FindTopicsNumber(
      books.dtm,
      topics = seq(1,10),
      metrics = "Arun2010",
      method = "Gibbs",
)

# Printresults
topics.results %>% arrange(Arun2010)

```

We want to sleect k that is the smallest as the Arun2010 metric measure the K-L divergence and this value is higher for non-optimal values of k [7]. The lowest k is 8. We can run our LDA model again with k-8 and look at the top word probabilities again.

```{r}
books.lda.optimal <- LDA(books.dtm, k = 8, control = list(seed = 555))

books.topics.optimal <- tidy(books.lda.optimal, matrix = "beta")
words.top.optimal <- books.topics.optimal %>% 
   group_by(topic) %>%
  top_n(7, beta) %>%
  ungroup() %>%
   mutate(term = reorder_within(term, beta, topic))

ggplot(words.top.optimal, aes(beta, term, fill = factor(topic))) +
  geom_col(show.legend = FALSE) +
 facet_wrap(~ topic, scales = "free") +
 scale_y_reordered()

```

We can also look at the composition of topics in each document. Since each document (book) is written by a different author (and not a mixture of authors), we expect that one topic for each book should stand out above the rest.

```{r}
books.docs.optimal <- tidy(books.lda.optimal, matrix = "gamma")
books.docs.optimal

books.docs.optimal %>%
  mutate(document= reorder(document, gamma * topic)) %>%
  ggplot(aes(factor(topic), gamma)) +
  geom_boxplot() +
  facet_wrap(~ document) +
  labs(x = "topic", y = expression(gamma))
```

Let's compare the number of unique authors in our set to the k we selected.

```{r}

books.selection %>% distinct(author) %>% count()

```


# TODO:
# - Revise LDA section
# - Update graphics to make new ones rather than following tidy text
# - Finish the example lda. Discuss if it grouped books into comon authors

```


# References

1. [Text Mining with R](https://www.tidytextmining.com/index.html)

2. [Project Gutenberg](https://www.gutenberg.org/)

3. [Roman Numerals with Regex](https://www.oreilly.com/library/view/regular-expressions-cookbook/9780596802837/ch06s09.html)

4. [Sentiment Datasets](https://www.datacamp.com/community/tutorials/sentiment-analysis-R)

5. [Latent Dirichlet Allocation](http://www.cse.cuhk.edu.hk/irwin.king/_media/presentations/latent_dirichlet_allocation.pdf)

6. [TF-IDF](https://cran.r-project.org/web/packages/tidytext/vignettes/tf_idf.html) 

7. [Loughran Sentiment Dataset](https://www.rdocumentation.org/packages/tidytext/versions/0.2.0/topics/sentiments)

8. [On Finding the Natural Number of Topics with Latent Dirichlet Allocation] (https://doi.org/10.1007/978-3-642-13657-3_43)
