---
title: "Intro to Text Mining"
author: "Pantea Ferdosian, Kevin Hoffman, Luke Moles, Marissa Shand"
output:
 html_document:
   toc: TRUE
   theme: united
   toc_depth: 3
   number_sections: TRUE
   df_print: paged
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Text Mining Motivation

#### **Need for Text Mining**:

The amount of data that we produce every day is truly astonishing. With the evolution of communication through social media, we generate tons and tons of data. Out of all of this generated data, only around $20\%$ are Structured and well formatted. The remaining of the data is Unstructured Data For instance, the bulk of emails we send, the text messages and the comments in social media are examples of Unstructured Data.
These Unstructured data may be analyzed and mined in order to extract useful information. 


#### What is "Text Mining" and where is it used? 

Text Mining is basically the process of extracting meaningful information from natural language text. 

A few examples of how we use text mining every day:

* Auto-complete feature
* Spam Detection
* Predictive Typing
* Spell Checker
    
####  Terminologies In Text Mining:

**Tokenization: ** The process of splitting the whole data (corpus) into smaller chunks is known as tokenization. (i.e. sentences into words)

**Remove Stop Words:** Stop words that are words that are very commonly used such as "although", "really", "from", etc. By removing such words in a language, we may focus on the important words instead.

**Document Term Matrix or DTM:** A matrix containing the terms that appear in a collection of documents and the frequency of the terms, where rows represent the documents and columns represent the words (terms). 


# Working with text data

"A **token** is a meaningful unit of text, such as wa word, that we are interested in using for analysis, and tokenization is the process of splitting test into tokens" [1]. Can be a word, n-gram, sentence or paragraph.

## Packages we will be using 

```{r, warning = FALSE, message = FALSE}
library(tidyverse)
library(tidytext)
library(gutenbergr)
```

## Exploring Project Gutenberg

"Project Gutenberg is an online library of free eBooks. Project Gutenberg was the first provider of free electronic books, or eBooks." [2]

```{r}
## Find books by Charles Dickens
dickens <- gutenberg_works(author == 'Dickens, Charles')
dim(dickens)
head(dickens)

## We will be working with A Tale of Two Cities, which has id 98
two_cities <- gutenberg_download(98)
head(two_cities)

## There are three books that make up this book
## Get book number
two_cities <- two.cities %>% mutate(book = cumsum(str_detect(text, regex("^Book the"))),
                                    linenumber = row_number())

## For each book get the linenumber in the book, and the chapter
## Roman numerals: https://www.oreilly.com/library/view/regular-expressions-cookbook/9780596802837/ch06s09.html
two_cities <- two_cities %>% group_by(book) %>% 
   mutate(book_linenumber = row_number(),
          chapter = cumsum(str_detect(text, regex("^(?=[MDCLXVI])M*(C[MD]|D?C{0,3})(X[CL]|L?X{0,3})(I[XV]|V?I{0,3})[.]")))) %>%
   ungroup()

## Convert to tidy text
tidy_two_cities <- two_cities %>% unnest_tokens(word, text)
head(tidy_two_cities)
```
## Stop words

```{r}
## Error arose: https://stackoverflow.com/questions/9221310/r-debugging-only-0s-may-be-mixed-with-negative-subscripts
data(stop_words)
stop_words

tidy_two_cities %>% anti_join(stop_words, by = c("word" = "word"))
```


# Sentiment Analysis

## Sentiment Lexicons 

tidytext provides three general purpose lexicons:

1. AFINN

2. Bing

3. NRC

# TF-IDF

**How should quantify what a document is about?**


* TF term is short for *term frequency* which indicates how frequently a word appears in a document. The document would most likely include unimportant words which we classified earlier as *stop words*, but removing the stop words would not be an efficient approach in this case because some of these words might be more important in some documents than others. 

* IDF term or *inverse document frequency* defined as :$idf(term) = ln(\frac{n_{documenta}}{n_{documents\ containing\ term}})$. This method adjust the weights and balances the usage of words in documents. This implies that it decreases the weight for widely used words and increases the weight for words that did not appear a lot.

* TF-IDF is short for *term frequencyâ€“inverse document frequency*, which is a statistical measure that indicates how important a word is to a document in a collection. This is obtained by multiplying the 2 mentioned quantities, which would indicate "the frequency of a term adjusted for how rarely it is used".

CODE TO BE ADDED HERE

# Relationships between words: n-grams and correlations

# Latent Direchlet Allocation

- Intro to Topic Modeling
   - similar to clustering numeric data
   - topics are clusters of words
   - goal is to discover latent patterns in documents
   - Examples
      - Apply it to a large batch of emails to understand what topics were discussed
      - Presidential speeches to identify themes
      - Collection of tweets from a group to identify what topics people tweet about
   - Latent Dirichlet allocation is an unsupervised method for finding topics in a collection of documents
      - probabilistic generative model
      - every document is a collection of topics
      - every topic is a collection of words
      - Documents are a mixture of topics ie document 1 is 60% of topic 1 and 40% of topic 2
      - Topics are a mixture of words ie 10% apples, 5% oranges, etc (maybe the topic is fruit)
   - Algorithm
      - From the paper:
         - Choose N from a poisson distribution
         - Choose $\Theta$ from a multinomial distribution
         - For each of the N words
            - Choose a topic $z_n$ from the multinomial($\theta$) distribution
            - Choose a word $w_n$ from $p(w_n|z_n,\beta)$, the probability of a word given the document
      - In easier terms maybe:
         - first, select k number of topics
         - randomly assign each word in each document in each topic
         - calculate proportion of words in document assigned to a topic
         - calculate proportion of words assigned to topic across all topics
         - Reassign word to new topic by using gibbes to sample the posterior
         - Repeat sampling for a number of draws
   - LDA Example: Randomly select 20 books from the top 10 authors gutenburg. How many authors are present?
      - authors are topics
      - books are documents
      - Tune k with https://cran.r-project.org/web/packages/ldatuning/vignettes/topics.html
         - Rather than fitting 10 models (k=1..10), use this package
         - Discuss hyperparemeter tuning
      - Identify and discuss one of the 4 criteria for evaluating each LDA model

## LDA Example
```{r}
library(gutenbergr)
# Set of popular authors
authors.popular <-
   c(
      "Dickens, Charles",
      "Austen, Jane",
      "Shelley, Mary Wollstonecraft",
      "Twain, Mark",
      "Doyle, Arthur Conan",
      "Wilde, Oscar",
      "Leech, John",
      "Hawthorne, Nathaniel",
      "Stevenson, Robert Louis",
      "Carroll, Lewis"
   )

# Download all books by these authors
books.authors.popular <- gutenberg_metadata %>%
   filter(
      author %in% authors.popular,
      language == "en", # Only english
      !str_detect(title, "Works"),  # Ignore collections of works
      has_text,
      !str_detect(rights, "Copyright")
   ) %>%
   distinct(title, .keep_all = TRUE) %>%
   select(gutenberg_id, title)

# Select a random sample of 20
books.selection <- books.metadata %>% sample_n(20)

# Download the 20 books
books.list <-
   books.selection$gutenberg_id %>% gutenberg_download(meta_fields = "title")

# Clean the text by removing blank rows
books.text <- books.list %>%
   filter(text != '') %>% # Remove blank lines
   select(-gutenberg_id) %>% # Drop the id
   group_by(title)  %>%
   unite(document, title)

words.by.book <- books.text %>%
   unnest_tokens(word, text)

# Generate word counts for each word in our documents
word.counts <- words.by.book %>%
   anti_join(stop_words) %>%
   count(document, word, sort = TRUE) %>%
   ungroup()

# Create a Document Term Matrix
books_dtm <- word.counts %>%
   cast_dtm(document, word, n)

# Try a model with 5 authors
books.lda <- LDA(books_dtm, k = 5, control = list(seed = 555))

# TODO:
# - Show some visualizations of words and topics and the associated probabilities
# - Tune k with ldatuning package
# - Show results and evaluate if we chose the correct k

```


# References

1. [Text Mining with R](https://www.tidytextmining.com/index.html)

2. [Project Gutenberg](https://www.gutenberg.org/)

3. [Roman Numerals with Regex](https://www.oreilly.com/library/view/regular-expressions-cookbook/9780596802837/ch06s09.html)

4. [Sentiment Datasets](https://www.datacamp.com/community/tutorials/sentiment-analysis-R)

5. [Latent Dirichlet Allocation](http://www.cse.cuhk.edu.hk/irwin.king/_media/presentations/latent_dirichlet_allocation.pdf)

6. [TF-IDF] (https://cran.r-project.org/web/packages/tidytext/vignettes/tf_idf.html) 
